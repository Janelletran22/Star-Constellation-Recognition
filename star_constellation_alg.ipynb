{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# constellation detection system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please prepare your dataset by placing images and labels in the appropriate directories\n",
      "Created YAML configuration at stars_constellations_dataset/constellation_data.yaml\n",
      "Training YOLOv8s model...\n",
      "New https://pypi.org/project/ultralytics/8.3.113 available  Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.112  Python-3.11.5 torch-2.6.0+cu118 CUDA:0 (NVIDIA GeForce GTX 1070, 8192MiB)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0mtask=detect, mode=train, model=yolov8s.pt, data=stars_constellations_dataset/constellation_data.yaml, epochs=10, time=None, patience=100, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=0, workers=8, project=None, name=constellation_detector5, exist_ok=False, pretrained=True, optimizer=Adam, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, conf=0.25, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.0005, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, cfg=None, tracker=botsort.yaml, save_dir=runs\\detect\\constellation_detector5\n",
      "Overriding model.yaml nc=80 with nc=16\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       928  ultralytics.nn.modules.conv.Conv             [3, 32, 3, 2]                 \n",
      "  1                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  2                  -1  1     29056  ultralytics.nn.modules.block.C2f             [64, 64, 1, True]             \n",
      "  3                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  4                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  5                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  6                  -1  2    788480  ultralytics.nn.modules.block.C2f             [256, 256, 2, True]           \n",
      "  7                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n",
      "  8                  -1  1   1838080  ultralytics.nn.modules.block.C2f             [512, 512, 1, True]           \n",
      "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    591360  ultralytics.nn.modules.block.C2f             [768, 256, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 16                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 19                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1   1969152  ultralytics.nn.modules.block.C2f             [768, 512, 1]                 \n",
      " 22        [15, 18, 21]  1   2122240  ultralytics.nn.modules.head.Detect           [16, [128, 256, 512]]         \n",
      "Model summary: 129 layers, 11,141,792 parameters, 11,141,776 gradients, 28.7 GFLOPs\n",
      "\n",
      "Transferred 349/355 items from pretrained weights\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n.pt to 'yolo11n.pt'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5.35M/5.35M [00:00<00:00, 5.61MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed \n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access  (ping: 0.10.0 ms, read: 5.41.2 MB/s, size: 35.9 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\Users\\Dell\\Downloads\\Star Constellation\\stars_constellations_dataset\\labels\\train.cache... 1530 images, 0 backgrounds, 0 corrupt: 100%|██████████| 1530/1530 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.20.1 ms, read: 3.80.6 MB/s, size: 37.7 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\Dell\\Downloads\\Star Constellation\\stars_constellations_dataset\\labels\\val.cache... 146 images, 0 backgrounds, 0 corrupt: 100%|██████████| 146/146 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs\\detect\\constellation_detector5\\labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m Adam(lr=0.0005, momentum=0.937) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1mruns\\detect\\constellation_detector5\u001b[0m\n",
      "Starting training for 10 epochs...\n",
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       1/10      4.23G       2.42      3.787      2.002         37        640: 100%|██████████| 96/96 [00:50<00:00,  1.88it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:01<00:00,  4.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        146        561      0.332     0.0685      0.195        0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       2/10      4.25G      1.863      1.378      1.567         41        640: 100%|██████████| 96/96 [00:49<00:00,  1.95it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:01<00:00,  3.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        146        561      0.777       0.69      0.773      0.441\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       3/10      4.25G      1.745      1.112      1.476         38        640: 100%|██████████| 96/96 [00:48<00:00,  1.96it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:01<00:00,  3.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        146        561      0.798      0.761      0.811      0.472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       4/10      4.25G      1.683     0.9896      1.448         28        640: 100%|██████████| 96/96 [00:48<00:00,  1.96it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:01<00:00,  3.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        146        561      0.843      0.744      0.807      0.474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       5/10      4.25G      1.585      0.884      1.373         23        640: 100%|██████████| 96/96 [00:48<00:00,  1.97it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:01<00:00,  3.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        146        561      0.832        0.9      0.913      0.534\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       6/10      4.25G      1.546       0.82      1.357         41        640: 100%|██████████| 96/96 [00:48<00:00,  1.97it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:01<00:00,  3.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        146        561      0.874      0.895       0.91      0.533\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       7/10      4.25G      1.511     0.7726       1.34         34        640: 100%|██████████| 96/96 [00:48<00:00,  1.96it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:01<00:00,  3.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        146        561      0.874      0.891      0.908      0.539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       8/10      4.25G      1.458       0.72      1.311         43        640: 100%|██████████| 96/96 [00:48<00:00,  1.98it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:01<00:00,  3.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        146        561      0.873      0.918      0.923      0.552\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       9/10      4.25G      1.408     0.6728      1.271         40        640: 100%|██████████| 96/96 [00:48<00:00,  1.97it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:01<00:00,  3.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        146        561      0.877       0.93       0.94      0.574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      10/10      4.25G      1.386     0.6511      1.265         33        640: 100%|██████████| 96/96 [00:49<00:00,  1.96it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:01<00:00,  3.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        146        561       0.88      0.932      0.942      0.559\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "10 epochs completed in 0.152 hours.\n",
      "Optimizer stripped from runs\\detect\\constellation_detector5\\weights\\last.pt, 22.5MB\n",
      "Optimizer stripped from runs\\detect\\constellation_detector5\\weights\\best.pt, 22.5MB\n",
      "\n",
      "Validating runs\\detect\\constellation_detector5\\weights\\best.pt...\n",
      "Ultralytics 8.3.112  Python-3.11.5 torch-2.6.0+cu118 CUDA:0 (NVIDIA GeForce GTX 1070, 8192MiB)\n",
      "Model summary (fused): 72 layers, 11,131,776 parameters, 0 gradients, 28.5 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:01<00:00,  2.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        146        561      0.877      0.931      0.941      0.574\n",
      "                Aquila         28         28      0.966          1      0.995      0.669\n",
      "                Bootes         34         34      0.944          1      0.995      0.665\n",
      "           Canis Major         29         29      0.935          1      0.995      0.644\n",
      "           Canis Minor         35         35      0.886      0.886       0.93      0.419\n",
      "            Cassiopeia         49         49       0.86          1      0.976      0.587\n",
      "                Cygnus         35         35      0.739      0.971      0.972       0.72\n",
      "                Gemini         36         36          1      0.944      0.972      0.647\n",
      "                   Leo         30         30      0.882          1      0.987      0.663\n",
      "                  Lyra         41         41      0.755      0.976       0.97      0.488\n",
      "                  Moon         40         40      0.744        0.8      0.801      0.354\n",
      "                 Orion         32         32          1          1      0.995      0.709\n",
      "              Pleiades         51         51      0.675      0.529      0.588      0.167\n",
      "           Sagittarius         18         18      0.895      0.944      0.969      0.442\n",
      "                Taurus         27         27      0.926      0.926       0.95       0.65\n",
      "            Ursa Major         33         33      0.969      0.939      0.969      0.625\n",
      "                  Moon         42         43      0.857      0.977      0.986      0.737\n",
      "Speed: 0.4ms preprocess, 4.9ms inference, 0.0ms loss, 1.8ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\constellation_detector5\u001b[0m\n",
      "Evaluating model performance...\n",
      "Ultralytics 8.3.112  Python-3.11.5 torch-2.6.0+cu118 CUDA:0 (NVIDIA GeForce GTX 1070, 8192MiB)\n",
      "Model summary (fused): 72 layers, 11,131,776 parameters, 0 gradients, 28.5 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.10.0 ms, read: 416.676.0 MB/s, size: 47.1 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\Dell\\Downloads\\Star Constellation\\stars_constellations_dataset\\labels\\val.cache... 146 images, 0 backgrounds, 0 corrupt: 100%|██████████| 146/146 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 10/10 [00:02<00:00,  4.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        146        561      0.877      0.928       0.94      0.574\n",
      "                Aquila         28         28      0.966          1      0.994      0.668\n",
      "                Bootes         34         34      0.944          1      0.995      0.668\n",
      "           Canis Major         29         29      0.935          1      0.995      0.644\n",
      "           Canis Minor         35         35      0.886      0.886       0.93      0.416\n",
      "            Cassiopeia         49         49       0.86          1      0.976      0.587\n",
      "                Cygnus         35         35      0.739      0.971      0.971      0.724\n",
      "                Gemini         36         36          1      0.944      0.972      0.647\n",
      "                   Leo         30         30      0.882          1      0.987      0.663\n",
      "                  Lyra         41         41      0.755      0.976       0.97      0.489\n",
      "                  Moon         40         40      0.762        0.8      0.803      0.362\n",
      "                 Orion         32         32          1          1      0.995      0.712\n",
      "              Pleiades         51         51      0.658       0.49      0.575       0.17\n",
      "           Sagittarius         18         18      0.895      0.944      0.969       0.43\n",
      "                Taurus         27         27      0.926      0.926       0.95      0.651\n",
      "            Ursa Major         33         33      0.969      0.939      0.969      0.621\n",
      "                  Moon         42         43      0.857      0.977      0.986      0.735\n",
      "Speed: 0.3ms preprocess, 6.5ms inference, 0.0ms loss, 1.3ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\constellation_detector52\u001b[0m\n",
      "\n",
      "Evaluation Metrics:\n",
      "mAP50: 0.9397\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported format string passed to numpy.ndarray.__format__",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 206\u001b[0m\n\u001b[0;32m    203\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConstellation detection model training and evaluation complete!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 206\u001b[0m     main()\n",
      "Cell \u001b[1;32mIn[2], line 192\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    190\u001b[0m \u001b[38;5;66;03m# Step 4: Evaluate the model\u001b[39;00m\n\u001b[0;32m    191\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluating model performance...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 192\u001b[0m metrics \u001b[38;5;241m=\u001b[39m evaluate_model(model)\n\u001b[0;32m    194\u001b[0m \u001b[38;5;66;03m# Save the trained model\u001b[39;00m\n\u001b[0;32m    195\u001b[0m model_save_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodels/constellation_detector_yolov8s.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "Cell \u001b[1;32mIn[2], line 139\u001b[0m, in \u001b[0;36mevaluate_model\u001b[1;34m(model)\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mEvaluation Metrics:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmAP50: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmAP50\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrecision: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprecision\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRecall: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrecall\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mF1 Score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf1\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported format string passed to numpy.ndarray.__format__"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import yaml\n",
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Configuration\n",
    "DATASET_PATH = \"stars_constellations_dataset\"  # Path to dataset\n",
    "IMG_SIZE = 640  # Target image size\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 0.0005\n",
    "MODEL_TYPE = \"yolov8s.pt\"  # Using the small variant (11.2M parameters)\n",
    "\n",
    "# Define constellation classes (16 classes as per requirements)\n",
    "CLASSES = [\n",
    "    \"Aquila\", \"Bootes\", \"Canis Major\", \"Canis Minor\", \"Cassiopeia\",\n",
    "    \"Cygnus\", \"Gemini\", \"Leo\", \"Lyra\", \"Moon\", \n",
    "    \"Orion\", \"Pleiades\", \"Sagittarius\", \"Taurus\", \"Ursa Major\", \"Moon\"\n",
    "]\n",
    "\n",
    "def create_dataset_structure():\n",
    "    \"\"\"\n",
    "    Create the necessary directory structure for YOLOv8 format\n",
    "    \"\"\"\n",
    "    os.makedirs(f\"{DATASET_PATH}/images/train\", exist_ok=True)\n",
    "    os.makedirs(f\"{DATASET_PATH}/images/val\", exist_ok=True)\n",
    "    os.makedirs(f\"{DATASET_PATH}/images/test\", exist_ok=True)\n",
    "    os.makedirs(f\"{DATASET_PATH}/labels/train\", exist_ok=True)\n",
    "    os.makedirs(f\"{DATASET_PATH}/labels/val\", exist_ok=True)\n",
    "    os.makedirs(f\"{DATASET_PATH}/labels/test\", exist_ok=True)\n",
    "    \n",
    "    return\n",
    "\n",
    "def preprocess_images(input_dir, output_dir):\n",
    "    \"\"\"\n",
    "    Preprocess images by resizing to 640x640 as per requirements\n",
    "    \"\"\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    image_files = [f for f in os.listdir(input_dir) if f.endswith(('.jpg', '.png', '.jpeg'))]\n",
    "    \n",
    "    for img_file in tqdm(image_files, desc=\"Preprocessing images\"):\n",
    "        img_path = os.path.join(input_dir, img_file)\n",
    "        img = cv2.imread(img_path)\n",
    "        \n",
    "        # Check if image loaded correctly\n",
    "        if img is None:\n",
    "            print(f\"Failed to load image: {img_path}\")\n",
    "            continue\n",
    "        \n",
    "        # Resize to square 640x640\n",
    "        img_resized = cv2.resize(img, (IMG_SIZE, IMG_SIZE))\n",
    "        \n",
    "        # Save the preprocessed image\n",
    "        output_path = os.path.join(output_dir, img_file)\n",
    "        cv2.imwrite(output_path, img_resized)\n",
    "    \n",
    "    return\n",
    "\n",
    "def split_dataset(image_list, labels_list, train_ratio=0.7, val_ratio=0.2, test_ratio=0.1):\n",
    "    \"\"\"\n",
    "    Split dataset into train, validation and test sets based on specified ratio\n",
    "    \"\"\"\n",
    "    # First split into train and temporary sets\n",
    "    train_images, temp_images, train_labels, temp_labels = train_test_split(\n",
    "        image_list, labels_list, test_size=(val_ratio + test_ratio), random_state=42\n",
    "    )\n",
    "    \n",
    "    # Split temporary set into validation and test sets\n",
    "    val_ratio_adjusted = val_ratio / (val_ratio + test_ratio)\n",
    "    val_images, test_images, val_labels, test_labels = train_test_split(\n",
    "        temp_images, temp_labels, test_size=(1 - val_ratio_adjusted), random_state=42\n",
    "    )\n",
    "    \n",
    "    return (train_images, train_labels), (val_images, val_labels), (test_images, test_labels)\n",
    "\n",
    "def create_yaml_config():\n",
    "    \"\"\"\n",
    "    Create YAML configuration file for YOLOv8 training\n",
    "    \"\"\"\n",
    "    config = {\n",
    "        'path': os.path.abspath(DATASET_PATH),\n",
    "        'train': 'images/train',\n",
    "        'val': 'images/val',\n",
    "        'test': 'images/test',\n",
    "        'names': {i: name for i, name in enumerate(CLASSES)}\n",
    "    }\n",
    "    \n",
    "    with open(f\"{DATASET_PATH}/constellation_data.yaml\", 'w') as f:\n",
    "        yaml.dump(config, f, default_flow_style=False)\n",
    "    \n",
    "    print(f\"Created YAML configuration at {DATASET_PATH}/constellation_data.yaml\")\n",
    "    return f\"{DATASET_PATH}/constellation_data.yaml\"\n",
    "\n",
    "def train_model(yaml_path):\n",
    "    \"\"\"\n",
    "    Train the YOLOv8s model using the dataset\n",
    "    \"\"\"\n",
    "    # Load a pretrained YOLOv8s model\n",
    "    model = YOLO(MODEL_TYPE)\n",
    "    \n",
    "    # Train the model\n",
    "    results = model.train(\n",
    "        data=yaml_path,\n",
    "        epochs=EPOCHS,\n",
    "        batch=BATCH_SIZE,\n",
    "        imgsz=IMG_SIZE,\n",
    "        optimizer=\"Adam\",  # As per requirements\n",
    "        lr0=LEARNING_RATE,  # Initial learning rate\n",
    "        conf=0.25,  # Detection confidence threshold\n",
    "        name=\"constellation_detector\",\n",
    "        device=0 if torch.cuda.is_available() else 'cpu'\n",
    "    )\n",
    "    \n",
    "    return model, results\n",
    "\n",
    "def evaluate_model(model):\n",
    "    \"\"\"\n",
    "    Evaluate model performance using mAP50 and other metrics\n",
    "    \"\"\"\n",
    "    # Validate the model (this will calculate mAP50 as specified in requirements)\n",
    "    results = model.val()\n",
    "    \n",
    "    metrics = {\n",
    "        \"mAP50\": float(results.box.map50),  # Convert to float\n",
    "        \"precision\": float(results.box.p),   # Convert to float\n",
    "        \"recall\": float(results.box.r),      # Convert to float\n",
    "        \"f1\": float(results.box.f1)          # Convert to float\n",
    "    }\n",
    "    \n",
    "    print(\"\\nEvaluation Metrics:\")\n",
    "    print(f\"mAP50: {metrics['mAP50']:.4f}\")\n",
    "    print(f\"Precision: {metrics['precision']:.4f}\")\n",
    "    print(f\"Recall: {metrics['recall']:.4f}\")\n",
    "    print(f\"F1 Score: {metrics['f1']:.4f}\")\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def detect_constellations(model, image_path):\n",
    "    \"\"\"\n",
    "    Perform constellation detection on a single image\n",
    "    \"\"\"\n",
    "    # Run inference\n",
    "    results = model.predict(image_path, conf=0.25)\n",
    "    \n",
    "    # Process results\n",
    "    result = results[0]\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Display results\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(result.plot())\n",
    "    plt.axis('off')\n",
    "    plt.title('Constellation Detection')\n",
    "    plt.show()\n",
    "    \n",
    "    # Print detections\n",
    "    for box in result.boxes:\n",
    "        class_id = int(box.cls[0].item())\n",
    "        confidence = box.conf[0].item()\n",
    "        coordinates = box.xyxy[0].tolist()\n",
    "        print(f\"Detected {CLASSES[class_id]} with confidence {confidence:.2f} at {coordinates}\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "def main():\n",
    "    # Step 1: Create dataset structure\n",
    "    create_dataset_structure()\n",
    "    \n",
    "    # Note: At this point you would need to:\n",
    "    # 1. Collect/prepare your 1,750 labeled constellation images\n",
    "    # 2. Convert annotations to YOLO format\n",
    "    # 3. Place images and labels in the appropriate directories\n",
    "    print(\"Please prepare your dataset by placing images and labels in the appropriate directories\")\n",
    "    \n",
    "    # Step 2: Create YAML configuration\n",
    "    yaml_path = create_yaml_config()\n",
    "    \n",
    "    # Step 3: Train the model\n",
    "    print(\"Training YOLOv8s model...\")\n",
    "    model, training_results = train_model(yaml_path)\n",
    "    \n",
    "    # Step 4: Evaluate the model\n",
    "    print(\"Evaluating model performance...\")\n",
    "    metrics = evaluate_model(model)\n",
    "    \n",
    "    # Save the trained model\n",
    "    model_save_path = \"models/constellation_detector_yolov8s.pt\"\n",
    "    os.makedirs(os.path.dirname(model_save_path), exist_ok=True)\n",
    "    model.export(format=\"onnx\")  # Export to ONNX format\n",
    "    print(f\"Model saved to {model_save_path}\")\n",
    "    \n",
    "    # For demonstration, you would use your own test image\n",
    "    # detect_constellations(model, \"path_to_test_image.jpg\")\n",
    "    \n",
    "    print(\"Constellation detection model training and evaluation complete!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constellation Detection Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "# Configuration\n",
    "MODEL_PATH = \"models/constellation_detector_yolov8s.pt\"  # Path to trained model\n",
    "TEST_DATA_PATH = \"stars_constellations_dataset/images/test\"  # Path to test images\n",
    "CONF_THRESHOLD = 0.25  # Confidence threshold for detection\n",
    "IOU_THRESHOLD = 0.5  # IoU threshold for NMS\n",
    "\n",
    "# Constellation classes (16 classes)\n",
    "CLASSES = [\n",
    "    \"Aquila\", \"Bootes\", \"Canis Major\", \"Canis Minor\", \"Cassiopeia\",\n",
    "    \"Cygnus\", \"Gemini\", \"Leo\", \"Lyra\", \"Moon\", \n",
    "    \"Orion\", \"Pleiades\", \"Sagittarius\", \"Taurus\", \"Ursa Major\", \"Moon\"\n",
    "]\n",
    "\n",
    "def load_model():\n",
    "    \"\"\"Load the trained YOLOv8s model\"\"\"\n",
    "    try:\n",
    "        model = YOLO(MODEL_PATH)\n",
    "        print(f\"Model loaded successfully from {MODEL_PATH}\")\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model: {e}\")\n",
    "        return None\n",
    "\n",
    "def evaluate_on_test_set(model):\n",
    "    \"\"\"Evaluate the model on the test dataset\"\"\"\n",
    "    if model is None:\n",
    "        print(\"Model not loaded. Cannot evaluate.\")\n",
    "        return\n",
    "    \n",
    "    results = model.val(data=f\"{os.path.dirname(TEST_DATA_PATH)}/data.yaml\")\n",
    "    \n",
    "    # Extract metrics\n",
    "    metrics = {\n",
    "        \"mAP50\": results.box.map50,  # Primary metric\n",
    "        \"precision\": results.box.p,   # Secondary metric\n",
    "        \"recall\": results.box.r,      # Secondary metric\n",
    "        \"f1\": results.box.f1          # Secondary metric (F1 Score)\n",
    "    }\n",
    "    \n",
    "    # Display metrics\n",
    "    print(\"\\nEvaluation Metrics:\")\n",
    "    print(f\"mAP50: {metrics['mAP50']:.4f}\")\n",
    "    print(f\"Precision: {metrics['precision']:.4f}\")\n",
    "    print(f\"Recall: {metrics['recall']:.4f}\")\n",
    "    print(f\"F1 Score: {metrics['f1']:.4f}\")\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def visualize_results(model, num_samples=5):\n",
    "    \"\"\"Visualize detection results on random test samples\"\"\"\n",
    "    if model is None:\n",
    "        print(\"Model not loaded. Cannot visualize.\")\n",
    "        return\n",
    "    \n",
    "    # Get test images\n",
    "    test_images = [os.path.join(TEST_DATA_PATH, f) for f in os.listdir(TEST_DATA_PATH) \n",
    "                  if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "    \n",
    "    if len(test_images) == 0:\n",
    "        print(\"No test images found.\")\n",
    "        return\n",
    "    \n",
    "    # Select random samples\n",
    "    if len(test_images) > num_samples:\n",
    "        test_samples = np.random.choice(test_images, num_samples, replace=False)\n",
    "    else:\n",
    "        test_samples = test_images\n",
    "    \n",
    "    # Create figure for visualization\n",
    "    fig, axes = plt.subplots(1, len(test_samples), figsize=(20, 4))\n",
    "    if len(test_samples) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    # Process each sample\n",
    "    for i, img_path in enumerate(test_samples):\n",
    "        # Run inference\n",
    "        results = model.predict(img_path, conf=CONF_THRESHOLD, iou=IOU_THRESHOLD)\n",
    "        result = results[0]\n",
    "        \n",
    "        # Get the original image for display\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Plot result\n",
    "        result_img = result.plot()\n",
    "        axes[i].imshow(result_img)\n",
    "        axes[i].set_title(f\"Sample {i+1}\")\n",
    "        axes[i].axis('off')\n",
    "        \n",
    "        # Print detections\n",
    "        detections = []\n",
    "        for box in result.boxes:\n",
    "            class_id = int(box.cls[0].item())\n",
    "            confidence = box.conf[0].item()\n",
    "            detections.append(f\"{CLASSES[class_id]}: {confidence:.2f}\")\n",
    "        \n",
    "        print(f\"Sample {i+1} detections:\", \", \".join(detections))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"constellation_detection_samples.png\")\n",
    "    plt.show()\n",
    "\n",
    "def plot_confusion_matrix(model):\n",
    "    \"\"\"Plot confusion matrix for classification results\"\"\"\n",
    "    if model is None:\n",
    "        print(\"Model not loaded. Cannot plot confusion matrix.\")\n",
    "        return\n",
    "    \n",
    "    # Get all test images\n",
    "    test_images = [os.path.join(TEST_DATA_PATH, f) for f in os.listdir(TEST_DATA_PATH) \n",
    "                  if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "    \n",
    "    # Get ground truth labels from file names or label files\n",
    "    true_labels = []\n",
    "    pred_labels = []\n",
    "    \n",
    "    for img_path in test_images:\n",
    "        # Get ground truth class (assuming filename contains class info)\n",
    "        # In real scenario, you would read this from label files\n",
    "        img_name = os.path.basename(img_path)\n",
    "        label_path = img_path.replace('images', 'labels').replace('.jpg', '.txt').replace('.png', '.txt')\n",
    "        \n",
    "        if os.path.exists(label_path):\n",
    "            with open(label_path, 'r') as f:\n",
    "                line = f.readline().strip()\n",
    "                if line:\n",
    "                    class_id = int(line.split(' ')[0])\n",
    "                    true_labels.append(class_id)\n",
    "                else:\n",
    "                    continue\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        # Run inference\n",
    "        results = model.predict(img_path, conf=CONF_THRESHOLD)\n",
    "        result = results[0]\n",
    "        \n",
    "        # Get highest confidence detection\n",
    "        if len(result.boxes) > 0:\n",
    "            # Find detection with highest confidence\n",
    "            confidences = [box.conf[0].item() for box in result.boxes]\n",
    "            max_conf_idx = np.argmax(confidences)\n",
    "            pred_class_id = int(result.boxes[max_conf_idx].cls[0].item())\n",
    "            pred_labels.append(pred_class_id)\n",
    "        else:\n",
    "            # No detection, skip this image\n",
    "            true_labels.pop()  # Remove the corresponding true label\n",
    "    \n",
    "    # Create confusion matrix\n",
    "    if len(true_labels) > 0 and len(pred_labels) > 0:\n",
    "        cm = confusion_matrix(true_labels, pred_labels, labels=range(len(CLASSES)))\n",
    "        \n",
    "        # Plot\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=CLASSES, yticklabels=CLASSES)\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('True')\n",
    "        plt.title('Confusion Matrix for Constellation Detection')\n",
    "        plt.savefig(\"confusion_matrix.png\")\n",
    "        plt.show()\n",
    "        \n",
    "        # Classification report\n",
    "        report = classification_report(true_labels, pred_labels, target_names=CLASSES)\n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(report)\n",
    "    else:\n",
    "        print(\"Not enough data to create confusion matrix.\")\n",
    "\n",
    "def visualize_loss_curves():\n",
    "    \"\"\"Visualize training loss curves from the results file\"\"\"\n",
    "    try:\n",
    "        # Location of results CSV (this path might vary based on your training output)\n",
    "        results_path = \"runs/detect/constellation_detector/results.csv\"\n",
    "        \n",
    "        if not os.path.exists(results_path):\n",
    "            print(f\"Results file not found at {results_path}\")\n",
    "            return\n",
    "        \n",
    "        # Load results\n",
    "        results = pd.read_csv(results_path)\n",
    "        \n",
    "        # Plot loss curves\n",
    "        plt.figure(figsize=(15, 10))\n",
    "        \n",
    "        # Plot box loss\n",
    "        plt.subplot(3, 1, 1)\n",
    "        plt.plot(results['epoch'], results['box_loss'], label='Box Loss', color='blue')\n",
    "        plt.title('Box Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "        \n",
    "        # Plot classification loss\n",
    "        plt.subplot(3, 1, 2)\n",
    "        plt.plot(results['epoch'], results['cls_loss'], label='Classification Loss', color='green')\n",
    "        plt.title('Classification Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "        \n",
    "        # Plot distribution focal loss (DFL)\n",
    "        plt.subplot(3, 1, 3)\n",
    "        plt.plot(results['epoch'], results['dfl_loss'], label='Distribution Focal Loss', color='red')\n",
    "        plt.title('Distribution Focal Loss (DFL)')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(\"training_loss_curves.png\")\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print(f\"Error plotting loss curves: {e}\")\n",
    "\n",
    "def main():\n",
    "    # Step 1: Load model\n",
    "    model = load_model()\n",
    "    \n",
    "    # Step 2: Evaluate on test set\n",
    "    metrics = evaluate_on_test_set(model)\n",
    "    \n",
    "    # Step 3: Visualize results on sample images\n",
    "    visualize_results(model, num_samples=5)\n",
    "    \n",
    "    # Step 4: Plot confusion matrix\n",
    "    plot_confusion_matrix(model)\n",
    "    \n",
    "    # Step 5: Visualize loss curves\n",
    "    visualize_loss_curves()\n",
    "    \n",
    "    print(\"Evaluation complete!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constellation Detection Inference Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import time\n",
    "\n",
    "# Configuration\n",
    "MODEL_PATH = \"models/constellation_detector_yolov8s.pt\"  # Path to trained model\n",
    "CONF_THRESHOLD = 0.25  # Confidence threshold for detection\n",
    "IOU_THRESHOLD = 0.5    # IoU threshold for non-maximum suppression\n",
    "CLASSES = [\n",
    "    \"Aquila\", \"Bootes\", \"Canis Major\", \"Canis Minor\", \"Cassiopeia\",\n",
    "    \"Cygnus\", \"Gemini\", \"Leo\", \"Lyra\", \"Moon\", \n",
    "    \"Orion\", \"Pleiades\", \"Sagittarius\", \"Taurus\", \"Ursa Major\", \"Moon\"\n",
    "]\n",
    "\n",
    "def load_model():\n",
    "    \"\"\"Load the YOLOv8s model for inference\"\"\"\n",
    "    try:\n",
    "        model = YOLO(MODEL_PATH)\n",
    "        print(f\"Model loaded successfully from {MODEL_PATH}\")\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model: {e}\")\n",
    "        return None\n",
    "\n",
    "def process_image(model, image_path):\n",
    "    \"\"\"Process a single image and display results\"\"\"\n",
    "    if model is None:\n",
    "        print(\"Model not loaded. Cannot process image.\")\n",
    "        return None\n",
    "    \n",
    "    # Load image\n",
    "    try:\n",
    "        image = cv2.imread(image_path)\n",
    "        if image is None:\n",
    "            print(f\"Failed to load image: {image_path}\")\n",
    "            return None\n",
    "        \n",
    "        # Convert to RGB for display\n",
    "        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Run inference\n",
    "        start_time = time.time()\n",
    "        results = model.predict(image_path, conf=CONF_THRESHOLD, iou=IOU_THRESHOLD)\n",
    "        inference_time = time.time() - start_time\n",
    "        \n",
    "        result = results[0]\n",
    "        \n",
    "        # Create result image\n",
    "        result_img = result.plot()\n",
    "        \n",
    "        # Print detection information\n",
    "        print(f\"\\nProcessed image: {image_path}\")\n",
    "        print(f\"Inference time: {inference_time:.4f} seconds\")\n",
    "        print(\"Detections:\")\n",
    "        \n",
    "        for i, box in enumerate(result.boxes):\n",
    "            class_id = int(box.cls[0].item())\n",
    "            confidence = box.conf[0].item()\n",
    "            coordinates = box.xyxy[0].tolist()  # x1, y1, x2, y2 format\n",
    "            \n",
    "            print(f\"  {i+1}. {CLASSES[class_id]}: {confidence:.4f} at {[round(c, 2) for c in coordinates]}\")\n",
    "        \n",
    "        return result_img\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing image: {e}\")\n",
    "        return None\n",
    "\n",
    "def process_directory(model, directory_path, output_dir=\"output\"):\n",
    "    \"\"\"Process all images in a directory and save results\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "testing cuda installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dell\\anaconda3\\Lib\\site-packages\\torch\\utils\\_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.6.0+cu118\n",
      "CUDA available: True\n",
      "CUDA version: 11.8\n",
      "GPU device name: NVIDIA GeForce GTX 1070\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU device name: {torch.cuda.get_device_name(0)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
